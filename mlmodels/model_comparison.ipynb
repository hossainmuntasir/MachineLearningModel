{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python version should be `3.11`.\n",
    "\n",
    "Scikit-learn version should be `1.5.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is 3.11.9\n",
      "Scikit-learn version is 1.5.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import platform\n",
    "\n",
    "print(f\"Python version is {platform.python_version()}\")\n",
    "print(f\"Scikit-learn version is {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 1 has 376821 out of 2097150 valid rows.\n",
      "Building 2 has 22716 out of 520938 valid rows.\n",
      "Building 3 has 467517 out of 1942515 valid rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_parquet(\"validated_data/cleaned1.parquet\")\n",
    "df2 = pd.read_parquet(\"validated_data/cleaned2.parquet\")\n",
    "df3 = pd.read_parquet(\"validated_data/cleaned3.parquet\")\n",
    "\n",
    "dfs = [df1, df2, df3]\n",
    "for i, df in enumerate(dfs):\n",
    "    dfs[i] = df[df[\"Faulty\"] == False]\n",
    "    print(f\"Building {i+1} has {len(dfs[i])} out of {len(df)} valid rows.\")\n",
    "df1, df2, df3 = dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2.groupby(\"Zone_name\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep columns with useful information.\n",
    "- `Slab_temp` and `Dew_temp` values do not vary in building 1\n",
    "- `building_no` is the same value within dataframes\n",
    "- `Fan_on_group` and `Cumulative_fan_on_mins` values were calculated using the target variable `Fan_status`\n",
    "- `Date` and `Time` cannot be easily converted to numeric values\n",
    "- `Year` values should probably not be used for prediction\n",
    "- `Damper_open_group` and `Louver_open_group` are categorical labels\n",
    "- Cumulative metrics probably won't be available when trying to predict future values\n",
    "- `Louver_status` values are missing in building 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\"building_no\", \"Fan_on_group\", \"Date\", \"Time\", \"Year\", \"Damper_open_group\", \"Louver_open_group\", \"Faulty\", \"Cumulative_fan_on_mins\", \"Cumulative_damper_open_mins\", \"Cumulative_louver_open_mins\"]\n",
    "exclude_df1 = exclude + [\"Slab_temp\", \"Dew_temp\", \"Slab_temp_diff\", \"Dew_temp_diff\"]\n",
    "exclude_df3 = exclude + [\"Louver_status\"]\n",
    "\n",
    "df1 = df1[df1.columns.difference(exclude_df1)]\n",
    "df2 = df2[df2.columns.difference(exclude)]\n",
    "df3 = df3[df3.columns.difference(exclude_df3)]\n",
    "dfs = [df1, df2, df3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Datetime` to get day of year and minutes past midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    df[\"Day_of_Year\"] = df[\"Datetime\"].dt.day_of_year\n",
    "    df[\"Minutes_past_midnight\"] = ((df[\"Datetime\"] - df[\"Datetime\"].dt.normalize()) / pd.Timedelta(minutes=1))\n",
    "    #df[\"DOW\"] = df[\"Datetime\"].dt.day_name().astype(\"category\")\n",
    "    del df[\"Datetime\"]\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.heatmap(df1[df1.columns.difference([\"Fan_status\", \"Zone_name\", \"DOW\"])].corr(), cmap=\"BrBG\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with `NA` values for columns that do not have many `NA` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18841\n",
      "Ambient_temp           3110\n",
      "Ambient_temp_diff      6074\n",
      "Damper_status        363206\n",
      "dtype: int64\n",
      "1135\n",
      "Ambient_temp_diff       1\n",
      "Zone_c02             1762\n",
      "dtype: int64\n",
      "23375\n",
      "Ambient_temp_diff     39\n",
      "Datetime_diff_mins    13\n",
      "Dew_temp_diff         48\n",
      "Slab_temp_diff        38\n",
      "Zone_temp_diff        41\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for df in dfs:\n",
    "    # limit to columns with less than 5% NA values\n",
    "    cutoff = len(df) // 20\n",
    "\n",
    "    sums = df.isna().sum()\n",
    "    sums = sums[sums != 0]\n",
    "    #print(cutoff)\n",
    "    #print(sums)\n",
    "    sums = sums[sums <= cutoff]\n",
    "    df.dropna(subset=sums.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform `Zone_name` from categorical to numeric. We'll use scikit-learn preprocessing for encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "\n",
    "def encode(categorical_columns: list[int] | list[int]):\n",
    "    return make_column_transformer((TargetEncoder(random_state=42), categorical_columns), remainder=\"passthrough\")\n",
    "\n",
    "#categorical_columns = [\"Zone_name\"]\n",
    "#categorical_columns = [\"Zone_name\", \"DOW\"]\n",
    "#df1 = pd.get_dummies(df1, columns=categorical_columns)\n",
    "#dfs[0] = df1\n",
    "#df2 = pd.get_dummies(df2, columns=categorical_columns)\n",
    "#dfs[1] = df2\n",
    "#df3 = pd.get_dummies(df3, columns=categorical_columns)\n",
    "#dfs[2] = df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the processed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_folder = Path(\"output/data\")\n",
    "save_folder.mkdir(parents=True, exist_ok=True)\n",
    "for i, df in enumerate(dfs):\n",
    "    df.to_parquet(save_folder / f\"building{i+1}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets into training and testing subsets\n",
    "We use a 80% training, 10% validation, 10% testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train1, test1 = train_test_split(df1, test_size=0.2, random_state=42)\n",
    "val1, test1 = train_test_split(test1, test_size=0.5, random_state=42)\n",
    "train2, test2 = train_test_split(df2, test_size=0.2, random_state=42)\n",
    "val2, test2 = train_test_split(test2, test_size=0.5, random_state=42)\n",
    "train3, test3 = train_test_split(df3, test_size=0.2, random_state=42)\n",
    "val3, test3 = train_test_split(test3, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use random undersampling to handle unbalanced training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Fan_status\"\n",
    "\n",
    "for train in [train1, train2, train3]:\n",
    "    counts = train[target].value_counts()\n",
    "    if counts[\"On\"] > counts[\"Off\"] * 3:\n",
    "        # data is already in random order, so we can just remove the relevant records after the cutoff\n",
    "        drop = train[train[target] == \"On\"].index[counts[\"Off\"] * 3:]\n",
    "        train.drop(drop, inplace=True)\n",
    "    elif counts[\"Off\"] > counts[\"On\"] * 3:\n",
    "        drop = train[train[target] == \"Off\"].index[counts[\"On\"] * 3:]\n",
    "        train.drop(drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = df1.columns.to_list()\n",
    "features1.remove(target)\n",
    "y_train1 = train1[target]\n",
    "y_val1 = val1[target]\n",
    "y_test1 = test1[target]\n",
    "x_train1 = train1[features1]\n",
    "x_val1 = val1[features1]\n",
    "x_test1 = test1[features1]\n",
    "\n",
    "features2 = df2.columns.to_list()\n",
    "features2.remove(target)\n",
    "y_train2 = train2[target]\n",
    "y_val2 = val2[target]\n",
    "y_test2 = test2[target]\n",
    "x_train2 = train2[features2]\n",
    "x_val2 = val2[features2]\n",
    "x_test2 = test2[features2]\n",
    "\n",
    "features3 = df3.columns.to_list()\n",
    "features3.remove(target)\n",
    "y_train3 = train3[target]\n",
    "y_val3 = val3[target]\n",
    "y_test3 = test3[target]\n",
    "x_train3 = train3[features3]\n",
    "x_val3 = val3[features3]\n",
    "x_test3 = test3[features3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from joblib import dump, load\n",
    "\n",
    "def save_model(model: BaseEstimator, filename: str):\n",
    "    models_folder = Path(\"output/models\")\n",
    "    models_folder.mkdir(parents=True, exist_ok=True)\n",
    "    filename = filename + \".joblib\"\n",
    "    dump(model, models_folder / filename)\n",
    "\n",
    "def load_model(filename: str) -> BaseEstimator | None:\n",
    "    models_folder = Path(\"output/models\")\n",
    "    filename = filename + \".joblib\"\n",
    "    filename: Path = models_folder / filename\n",
    "    if filename.is_file():\n",
    "        #print(\"File found!\")\n",
    "        return load(filename)\n",
    "    #print(\"File does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score, average_precision_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def calculate_metrics(model: BaseEstimator, X: pd.Series, y: pd.Series, print_report = True):\n",
    "    y_predict = model.predict(X)\n",
    "    # SVM predictions take a long time to compute and are unreliable (https://stackoverflow.com/questions/15111408/how-does-sklearn-svm-svcs-function-predict-proba-work-internally)\n",
    "    #y_proba = model.predict_proba(X)[:,1]\n",
    "\n",
    "    if print_report:\n",
    "        print(classification_report(y, y_predict, digits=4))\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_predict).ravel()\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y, y_predict),\n",
    "        \"f1\": f1_score(y, y_predict, pos_label=\"On\"),\n",
    "        #\"ROC AUC\": roc_auc_score(y, y_proba),\n",
    "        #\"PR AUC\": average_precision_score(y, y_proba, pos_label=\"On\"),\n",
    "        \"Matthews\": matthews_corrcoef(y, y_predict),\n",
    "        \"TN\": tn,\n",
    "        \"FP\": fp,\n",
    "        \"FN\": fn,\n",
    "        \"TP\": tp,\n",
    "    }\n",
    "\n",
    "model_metrics = {f\"Building {n+1}\": {} for n in range(3)}\n",
    "tuned_metrics = {f\"Building {n+1}\": {} for n in range(3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree = make_pipeline(encode([\"Zone_name\"]), tree)\n",
    "model_name = \"Decision Tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(x_train1, y_train1)\n",
    "model_metrics[\"Building 1\"][model_name] = calculate_metrics(tree, x_val1, y_val1)\n",
    "tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(tree, x_test1, y_test1, print_report=False)\n",
    "save_model(tree, \"decisiontree_building1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(x_train2, y_train2)\n",
    "model_metrics[\"Building 2\"][model_name] = calculate_metrics(tree, x_val2, y_val2)\n",
    "tuned_metrics[\"Building 2\"][model_name + \" (default)\"] = calculate_metrics(tree, x_test2, y_test2, print_report=False)\n",
    "save_model(tree, \"decisiontree_building2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(x_train3, y_train3)\n",
    "model_metrics[\"Building 3\"][model_name] = calculate_metrics(tree, x_val3, y_val3)\n",
    "tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(tree, x_test3, y_test3, print_report=False)\n",
    "save_model(tree, \"decisiontree_building3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "\n",
    "model_name = \"Random Forest\"\n",
    "forest = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "forest = make_pipeline(encode([\"Zone_name\"]), forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fit(x_train1, y_train1)\n",
    "feature_names = forest[0].get_feature_names_out()\n",
    "feature_names = [feature.split(\"__\")[1] for feature in feature_names]\n",
    "sns.barplot(pd.DataFrame(forest[-1].feature_importances_, index=feature_names).T, orient=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics[\"Building 1\"][model_name] = calculate_metrics(forest, x_val1, y_val1)\n",
    "tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(forest, x_test1, y_test1, print_report=False)\n",
    "save_model(forest, \"randomforest_building1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fit(x_train2, y_train2)\n",
    "feature_names = forest[0].get_feature_names_out()\n",
    "feature_names = [feature.split(\"__\")[1] for feature in feature_names]\n",
    "sns.barplot(pd.DataFrame(forest[-1].feature_importances_, index=feature_names).T, orient=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics[\"Building 2\"][model_name] = calculate_metrics(forest, x_val2, y_val2)\n",
    "tuned_metrics[\"Building 2\"][model_name + \" (default)\"] = calculate_metrics(forest, x_test2, y_test2, print_report=False)\n",
    "save_model(forest, \"randomforest_building2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fit(x_train3, y_train3)\n",
    "feature_names = forest[0].get_feature_names_out()\n",
    "feature_names = [feature.split(\"__\")[1] for feature in feature_names]\n",
    "sns.barplot(pd.DataFrame(forest[-1].feature_importances_, index=feature_names).T, orient=\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics[\"Building 3\"][model_name] = calculate_metrics(forest, x_val3, y_val3)\n",
    "tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(forest, x_test3, y_test3, print_report=False)\n",
    "save_model(forest, \"randomforest_building3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram-based Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "model_name = \"Hist Gradient Boosting\"\n",
    "histgb = HistGradientBoostingClassifier(random_state=42, categorical_features=[\"Zone_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histgb.fit(x_train1, y_train1)\n",
    "model_metrics[\"Building 1\"][model_name] = calculate_metrics(histgb, x_val1, y_val1)\n",
    "tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(histgb, x_test1, y_test1, print_report=False)\n",
    "save_model(histgb, \"histgb_building1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histgb.fit(x_train2, y_train2)\n",
    "model_metrics[\"Building 2\"][model_name] = calculate_metrics(histgb, x_val2, y_val2)\n",
    "tuned_metrics[\"Building 2\"][model_name + \" (default)\"] = calculate_metrics(histgb, x_test2, y_test2, print_report=False)\n",
    "save_model(histgb, \"histgb_building2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histgb.fit(x_train3, y_train3)\n",
    "model_metrics[\"Building 3\"][model_name] = calculate_metrics(histgb, x_val3, y_val3)\n",
    "tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(histgb, x_test3, y_test3, print_report=False)\n",
    "save_model(histgb, \"histgb_building3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_name = \"Logistic Regression\"\n",
    "logit = LogisticRegression(random_state=42)\n",
    "logit = make_pipeline(encode([\"Zone_name\"]), SimpleImputer(), StandardScaler(), logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(x_train1, y_train1)\n",
    "model_metrics[\"Building 1\"][model_name] = calculate_metrics(logit, x_val1, y_val1)\n",
    "tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(logit, x_test1, y_test1, print_report=False)\n",
    "save_model(logit, \"logit_building1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(x_train2, y_train2)\n",
    "model_metrics[\"Building 2\"][model_name] = calculate_metrics(logit, x_val2, y_val2)\n",
    "tuned_metrics[\"Building 2\"][model_name + \" (default)\"] = calculate_metrics(logit, x_test2, y_test2, print_report=False)\n",
    "save_model(logit, \"logit_building2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.fit(x_train3, y_train3)\n",
    "model_metrics[\"Building 3\"][model_name] = calculate_metrics(logit, x_val3, y_val3)\n",
    "tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(logit, x_test3, y_test3, print_report=False)\n",
    "save_model(logit, \"logit_building3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model_name = \"Linear Discriminant Analysis\"\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda = make_pipeline(encode([\"Zone_name\"]), SimpleImputer(), lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(x_train1, y_train1)\n",
    "model_metrics[\"Building 1\"][model_name] = calculate_metrics(lda, x_val1, y_val1)\n",
    "tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(lda, x_test1, y_test1, print_report=False)\n",
    "save_model(lda, \"lda_building1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(x_train2, y_train2)\n",
    "model_metrics[\"Building 2\"][model_name] = calculate_metrics(lda, x_val2, y_val2)\n",
    "tuned_metrics[\"Building 2\"][model_name + \" (default)\"] = calculate_metrics(lda, x_test2, y_test2, print_report=False)\n",
    "save_model(lda, \"lda_building2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(x_train3, y_train3)\n",
    "model_metrics[\"Building 3\"][model_name] = calculate_metrics(lda, x_val3, y_val3)\n",
    "tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(lda, x_test3, y_test3, print_report=False)\n",
    "save_model(lda, \"lda_building3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model_name = \"Linear SVM\"\n",
    "svm = LinearSVC(random_state=42)\n",
    "svm = make_pipeline(encode([\"Zone_name\"]), SimpleImputer(), MinMaxScaler(), Normalizer(), svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(x_train1, y_train1)\n",
    "model_metrics[\"Building 1\"][model_name] = calculate_metrics(svm, x_val1, y_val1)\n",
    "tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test1, y_test1, print_report=False)\n",
    "save_model(svm, \"svm_linear_building1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(x_train2, y_train2)\n",
    "model_metrics[\"Building 2\"][model_name] = calculate_metrics(svm, x_val2, y_val2)\n",
    "tuned_metrics[\"Building 2\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test2, y_test2, print_report=False)\n",
    "save_model(svm, \"svm_linear_building2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(x_train3, y_train3)\n",
    "model_metrics[\"Building 3\"][model_name] = calculate_metrics(svm, x_val3, y_val3)\n",
    "tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test3, y_test3, print_report=False)\n",
    "save_model(svm, \"svm_linear_building3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with RBF Kernel\n",
    "\n",
    "SVM takes a long time for large datasets. Even after loading the saved model file, making predictions on the buildings took around 1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model_name = \"SVM\"\n",
    "svm = SVC(random_state=42)\n",
    "svm = make_pipeline(encode([\"Zone_name\"]), SimpleImputer(), MinMaxScaler(), Normalizer(), svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "#from sklearn.impute import SimpleImputer\n",
    "#from sklearn.svm import LinearSVC\n",
    "#from sklearn.kernel_approximation import Nystroem\n",
    "\n",
    "#model_name = \"SVM (RBF)\"\n",
    "#svm = LinearSVC(random_state=42)\n",
    "#svm = make_pipeline(encode([\"Zone_name\"]), SimpleImputer(), MinMaxScaler(), Normalizer(), Nystroem(), svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 1 SVM training took around 3 hours for me. We'll try loading the saved model file instead.\n",
    "model_file = \"svm_rbf_building1\"\n",
    "#if load_model(model_file):\n",
    "#    print(\"Loading previously saved model\")\n",
    "#    svm = load_model(model_file)\n",
    "#    model_metrics[\"Building 1\"][model_name] = calculate_metrics(svm, x_val1, y_val1)\n",
    "#    tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test1, y_test1, print_report=False)\n",
    "#else:\n",
    "#    print(\"Training model...\")\n",
    "#    svm.fit(x_train1, y_train1)\n",
    "#    model_metrics[\"Building 1\"][model_name] = calculate_metrics(svm, x_val1, y_val1)\n",
    "#    tuned_metrics[\"Building 1\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test1, y_test1, print_report=False)\n",
    "#    save_model(svm, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(x_train2, y_train2)\n",
    "model_metrics[\"Building 2\"][model_name] = calculate_metrics(svm, x_val2, y_val2)\n",
    "tuned_metrics[\"Building 2\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test2, y_test2, print_report=False)\n",
    "save_model(svm, \"svm_rbf_building2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building 3 SVM training took around 1 hour for me. We'll try loading the saved model file instead.\n",
    "model_file = \"svm_rbf_building3\"\n",
    "#if load_model(model_file):\n",
    "#    print(\"Loading previously saved model\")\n",
    "#    svm = load_model(model_file)\n",
    "#    model_metrics[\"Building 3\"][model_name] = calculate_metrics(svm, x_val3, y_val3)\n",
    "#    tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test3, y_test3, print_report=False)\n",
    "#else:\n",
    "#    print(\"Training model...\")\n",
    "#    svm.fit(x_train1, y_train1)\n",
    "#    model_metrics[\"Building 3\"][model_name] = calculate_metrics(svm, x_val3, y_val3)\n",
    "#    tuned_metrics[\"Building 3\"][model_name + \" (default)\"] = calculate_metrics(svm, x_test3, y_test3, print_report=False)\n",
    "#    save_model(svm, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance\n",
    "### Building 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_metrics[\"Building 1\"]).T.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_metrics[\"Building 2\"]).T.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model_metrics[\"Building 3\"]).T.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and evaluating all model variants takes a long time to complete from scratch. Data will be cached so that future runs will not take as long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmodel import SCORES\n",
    "scores = list(SCORES)\n",
    "save_folder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmodel import GridSearchDecisionTree\n",
    "model_name = \"Decision Tree\"\n",
    "#save_folder = \"output/decision_tree/analysis\"\n",
    "save_folder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = GridSearchDecisionTree(\"building1\", n_fits=-1)\n",
    "opt1.fit(x_train1, y_train1, x_val1, y_val1)\n",
    "if save_folder:\n",
    "    print(opt1.save_results(save_folder))\n",
    "\n",
    "df = opt1.results\n",
    "df = pd.DataFrame(df[scores], index=df[\"nodes\"])\n",
    "plt = sns.lineplot(df)\n",
    "plt.set_xlim(0, len(df))\n",
    "#plt.set_ylim(0.998, 1.0)\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_metrics[\"Building 1\"][model_name] = calculate_metrics(opt1.model, x_test1, y_test1)\n",
    "save_model(opt1.model, \"decisiontree_building1_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt2 = GridSearchDecisionTree(\"building2\", n_fits=-1)\n",
    "opt2.fit(x_train2, y_train2, x_val2, y_val2)\n",
    "if save_folder:\n",
    "    print(opt2.save_results(save_folder))\n",
    "\n",
    "df = opt2.results\n",
    "df = pd.DataFrame(df[scores], index=df[\"nodes\"])\n",
    "plt = sns.lineplot(df)\n",
    "plt.set_xlim(0, len(df))\n",
    "#plt.set_ylim(0.6, 1.0)\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_metrics[\"Building 2\"][model_name] = calculate_metrics(opt2.model, x_test2, y_test2)\n",
    "save_model(opt2.model, \"decisiontree_building2_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt3 = GridSearchDecisionTree(\"building3\", n_fits=-1)\n",
    "opt3.fit(x_train3, y_train3, x_val3, y_val3)\n",
    "if save_folder:\n",
    "    print(opt3.save_results(save_folder))\n",
    "\n",
    "df = opt3.results\n",
    "df = pd.DataFrame(df[scores], index=df[\"nodes\"])\n",
    "plt = sns.lineplot(df)\n",
    "plt.set_xlim(0, len(df))\n",
    "#plt.set_ylim(0.92, 1.0)\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_metrics[\"Building 3\"][model_name] = calculate_metrics(opt3.model, x_test3, y_test3)\n",
    "save_model(opt3.model, \"decisiontree_building3_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmodel import GridSearchRandomForest\n",
    "model_name = \"Random Forest\"\n",
    "#save_folder = \"output/random_forest/analysis\"\n",
    "save_folder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = GridSearchRandomForest(\"building1\", n_fits=-1)\n",
    "opt1.fit(x_train1, y_train1, x_val1, y_val1)\n",
    "if save_folder:\n",
    "    print(opt1.save_results(save_folder))\n",
    "\n",
    "opt1.model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = opt1.results\n",
    "df = pd.DataFrame(df[scores], index=df[\"min_samples_split\"])\n",
    "plt = sns.lineplot(df)\n",
    "#plt.set_xlim(0, df.index.max())\n",
    "#plt.set_ylim(0.98, 1.0)\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_metrics[\"Building 1\"][model_name] = calculate_metrics(opt1.model, x_test1, y_test1)\n",
    "save_model(opt1.model, \"randomforest_building1_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt2 = GridSearchRandomForest(\"building2\", n_fits=-1)\n",
    "opt2.fit(x_train2, y_train2, x_val2, y_val2)\n",
    "if save_folder:\n",
    "    print(opt2.save_results(save_folder))\n",
    "\n",
    "opt2.model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = opt2.results\n",
    "df = pd.DataFrame(df[scores], index=df[\"min_samples_split\"])\n",
    "plt = sns.lineplot(df)\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_metrics[\"Building 2\"][model_name] = calculate_metrics(opt2.model, x_test2, y_test2)\n",
    "save_model(opt2.model, \"randomforest_building2_tuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt3 = GridSearchRandomForest(\"building3\", n_fits=-1)\n",
    "opt3.fit(x_train3, y_train3, x_val3, y_val3)\n",
    "if save_folder:\n",
    "    print(opt3.save_results(save_folder))\n",
    "\n",
    "opt3.model[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = opt3.results\n",
    "df = pd.DataFrame(df[scores], index=df[\"min_samples_split\"])\n",
    "plt = sns.lineplot(df)\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_metrics[\"Building 3\"][model_name] = calculate_metrics(opt3.model, x_test3, y_test3)\n",
    "save_model(opt3.model, \"randomforest_building3_tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Model Performance\n",
    "### Building 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tuned_metrics[\"Building 1\"]).T.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tuned_metrics[\"Building 2\"]).T.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tuned_metrics[\"Building 3\"]).T.sort_values(\"f1\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
